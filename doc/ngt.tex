Given a text corpus we can compute its dictionary and word frequencies
with the command:

\begin{verbatim}
$> dict -i="gunzip -c train.gz" -o=train.dict -f=yes
\end{verbatim}

\noindent
For speech recognition applications, it  can be often the case to limit the LM dictionary  only to the top frequent, let us say,
10K words. We can obtain such a list by:
\begin{verbatim}
$> dict -i="gunzip -c train.gz" -o=top10k -pr=10000
\end{verbatim}


\noindent
{\bf Notice}: the list will also include the start/end-sentence symbols.\\

\noindent
An alternative pruning strategy is to filter out words occurring less or equal than a specified 
count. The following example removes the word occurring $\le$ 5 times and keeps
the top frequent 10K (at most) of the others:
\begin{verbatim}
$> dict -i="gunzip -c train.gz" -o=top10k5f -pr=10000 -pf=5
\end{verbatim}


\noindent
Statistics about the frequency of words inside a corpus can be gathered through  the command {\tt dict}
with the option {\tt -curve=yes}, while out-of-vocabulary rate statistics over a test set can be computed with
the option {\tt -TestFile=<sample>}.  The following example illustrates both features:
\begin{verbatim}
$> dict -i="gunzip -c train.gz"  -Curve=yes -TestFile=test

**************** DICTIONARY GROWTH CURVE ****************
Freq    Entries Percent       Freq    OOV onTest
>0      15058    100.00%      <1        5.01%
>1      10113     67.16%      <2        7.64%
>2      8057      53.51%      <3        8.89%
>3      6948      46.14%      <4        9.96%
>4      6207      41.22%      <5       10.72%
>5      5644      37.48%      <6       11.67%
>6      5205      34.57%      <7       12.18%
>7      4823      32.03%      <8       12.72%
>8      4523      30.04%      <9       13.47%
>9      4224      28.05%      <10      14.10%
*********************************************************

\end{verbatim}

\noindent
A new  $n$-gram table for the  limited dictionary can  be computed with {\tt ngt} by specifying 
the sub-dictionary:
\begin{verbatim}
$> ngt -i=train.www -sd=top10k -n=3 -o=train.10k.www -b=yes
\end{verbatim}
The command replaces  all words outside  top10K with  the special
out-of-vocabulary symbol {\tt \_unk\_}.

\noindent
Another useful feature of ngt is the merging of two $n$-gram tables. Assume that we have 
split our training corpus into files  {\tt text-a} and file {\tt text-b} and have computed $n$-gram 
tables for both files, we can merge them with the option {\tt -aug}:
\begin{verbatim}
$> ngt -i="gunzip -c text-a.gz" -n=3 -o=text-a.www -b=yes
$> ngt -i="gunzip -c text-b.gz" -n=3 -o=text-b.www -b=yes
$> ngt -i=text-a.www -aug=text-b.www -n=3 -o=text.www -b=yes
\end{verbatim}

\paragraph{Warning:} Note that if the concatenation of {\tt text-a.gz} and {\tt text-b.gz} is equal to {\tt train.gz} the resulting $n$-gram tables
{\tt text.www} and {\tt train.www} can slightly differ. This happens because during the construction of each single $n$-gram table few $n$-grams are automatically added to make it consistent for further computation.


